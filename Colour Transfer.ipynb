{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f625b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.models.feature_extraction as feat\n",
    "from torchvision.io import decode_image\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing import Process\n",
    "from multiprocessing import Pipe\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import copy\n",
    "from skimage import io,color\n",
    "#thanks to https://stackoverflow.com/questions/47411872/extract-multiple-windows-patches-from-an-image-array-as-defined-in-another-ar\n",
    "from skimage.util.shape import view_as_windows\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [16, 10] # matplotlib setting to control the size of display images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd75de62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81769f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "unloader = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfb8ffb",
   "metadata": {},
   "source": [
    "# Implementing Patch Matching Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f47f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNF:\n",
    "    #creates mapping from img_1 to img_2\n",
    "    def __init__(self,img_1,img_2,patch_size = 7, norm = False):\n",
    "        if patch_size%2 == 0:\n",
    "            print(\"Patch size should be odd. Increasing by 1.\")\n",
    "            patch_size = patch_size + 1\n",
    "        self.normalised = norm\n",
    "        self.patch_size = patch_size\n",
    "        self.source = img_1\n",
    "        self.dest = img_2\n",
    "        self.rows = self.source.shape[1]\n",
    "        self.cols = self.source.shape[2]\n",
    "        self.chans = self.source.shape[0]\n",
    "        #random coordinates\n",
    "        self.matchings = np.random.rand(self.rows,self.cols,2)\n",
    "        #place x,y will contain mapping x'y'\n",
    "        self.matchings[:,:,0] = self.matchings[:,:,0]*self.dest.shape[1]\n",
    "        self.matchings[:,:,1] = self.matchings[:,:,1]*self.dest.shape[2]\n",
    "        self.matchings = self.matchings.astype(int)\n",
    "        return\n",
    "    #src:https://stackoverflow.com/questions/13405956/convert-an-image-rgb-lab-with-python\n",
    "    def __rgbToLAB(self,image_tensor):\n",
    "        labImage = color.rgb2lab(image_tensor.numpy(force = True),channel_axis = 0)\n",
    "        labTensor = torch.Tensor(labImage)\n",
    "        return labTensor\n",
    "    \n",
    "    def __labToRGB(self,lab_tensor):\n",
    "        rgbImage = color.lab2rgb(lab_tensor.numpy(force = True), channel_axis = 0)\n",
    "        rgbTensor = torch.Tensor(rgbImage)\n",
    "        return rgbTensor\n",
    "        \n",
    "                          \n",
    "    #Distance metric between two patches. If one of the patches is at the corner, it accounts for this.\n",
    "    def __D(self,row,col,dest_row,dest_col):\n",
    "        dest_rows = self.dest.shape[1]\n",
    "        dest_cols = self.dest.shape[2]\n",
    "        \n",
    "        r = self.patch_size//2\n",
    "        \n",
    "#         r_left = min(r,col,dest_col)\n",
    "#         r_right = min(r,self.cols - (col + 1),dest_cols - (dest_col + 1))\n",
    "#         r_top = min(r,row,dest_row)\n",
    "#         r_bott = min(r,self.rows - (row + 1),dest_rows - (dest_row + 1))\n",
    "        \n",
    "        #the array is padded by r, so we need to shift each index up by r\n",
    "#         patch1 = self.sNorm[:,r + row - r_top:r + row + r_bott + 1,r + col - r_left : r + col + r_right + 1]\n",
    "        patch1 = self.sWin[:,row,col].squeeze()\n",
    "        p1s = patch1.shape\n",
    "#         patch2 = self.dNorm[:,r + dest_row - r_top : r + dest_row + r_bott + 1, r + dest_col - r_left : r + dest_col + r_right + 1]\n",
    "        patch2 = self.dWin[:,dest_row,dest_col].squeeze()\n",
    "        p2s = patch2.shape\n",
    "        if(patch1.shape != patch2.shape):\n",
    "            print(p1s,p2s)\n",
    "            print(\"Source dims: ({} rows by {} cols)\\nDest dims: ({} rows by {} cols)\".format(self.rows,self.cols,dest_rows,dest_cols))\n",
    "            print(\"source row: {}\\nsource col: {}\\ndest row: {}\\ndest col:{}\".format(row,col,dest_row,dest_col))\n",
    "            print(\"Source Patch Cols: [{} to {})\".format(col - r_left,col + r_right + 1))\n",
    "            print(\"Source Patch Rows: [{} to {})\".format(row - r_top,row + r_bott + 1))\n",
    "            print(\"Dest Patch Cols: [{} to {})\".format(dest_col - r_left,dest_col + r_right + 1))\n",
    "            print(\"Dest Patch Rows: [{} to {})\".format(dest_row - r_top,dest_row + r_bott + 1))\n",
    "            print(r_left,r_right,r_top,r_bott,self.source[row - r_top:row + r_bott + 1,row - r_left:row + r_right + 1])\n",
    "        \n",
    "\n",
    "        res = float(((patch1 - patch2)**2).sum())**0.5\n",
    "        return res\n",
    "    \n",
    "    def __D_vec(self,source_loc,dest_locs):\n",
    "        #vectorised version of the distance metric, for the random search step\n",
    "        #the patches given by scikit's window view for a given location have the patch's top left corner as the origin.\n",
    "        #However, this is already accounted for since dNorm and sNorm are padded by r\n",
    "        dest_patch_coords = (dest_locs[:,0].reshape(1,dest_locs.shape[0]),dest_locs[:,1].reshape(1,dest_locs.shape[0]))\n",
    "        \n",
    "        dest_patches = self.dWin[:,dest_patch_coords[0],dest_patch_coords[1]][0][0]\n",
    "        #print(dest_patches.shape)\n",
    "        #dest_patches = dest_patches.squeeze()\n",
    "        \n",
    "        source_patch = self.sWin[:,source_loc[0],source_loc[1]].squeeze()\n",
    "        \n",
    "        \n",
    "        dists = ((source_patch - dest_patches)**2).sum(axis = tuple(range(1,len(dest_patches.shape))))**0.5\n",
    "        #print(dists)\n",
    "        closest = np.argmin(dists)\n",
    "        return np.array(dest_locs[closest]).astype(int),float(dists[closest])\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def __step(self,row,col,alpha = 0.5,reverse_prop = False,threshold = 0):\n",
    "        #setup\n",
    "        dest_row = self.matchings[row,col,0]\n",
    "        dest_col = self.matchings[row,col,1]\n",
    "        #print(dest_row)\n",
    "        D_min = self.dists[row,col]\n",
    "        if(D_min == np.inf):\n",
    "            D_min = self.__D(row,col,dest_row,dest_col)\n",
    "            self.dists[row,col] = D_min\n",
    "        #propogation step: Compare to mappings near previous spot\n",
    "        candidate = None\n",
    "        \n",
    "        #positive one if reverse_prop,negative one if not\n",
    "        pm1 = (reverse_prop * 2) - 1\n",
    "        \n",
    "        if(not reverse_prop and (row!= 0)) or (reverse_prop and (row!= self.rows - 1)):\n",
    "            prev = self.matchings[row + pm1,col]\n",
    "            if (not reverse_prop and (prev[0] < self.dest.shape[1] - 1)) or (reverse_prop and (prev[0] > 0)):\n",
    "                candidate = self.__D(row,col,prev[0] - pm1,prev[1])\n",
    "                if(candidate <= D_min):\n",
    "                    D_min = candidate\n",
    "                    self.matchings[row,col,0] = prev[0] - pm1\n",
    "                    self.matchings[row,col,1] = prev[1]\n",
    "                    self.dists[row,col] = D_min\n",
    "                \n",
    "        \n",
    "        if(not reverse_prop and (col!= 0)) or (reverse_prop and (col!= self.cols - 1)):\n",
    "            prev = self.matchings[row,col + pm1]\n",
    "            if (not reverse_prop and (prev[1] < self.dest.shape[2] - 1)) or (reverse_prop and (prev[1] > 0)):\n",
    "                candidate = self.__D(row,col,prev[0],prev[1] - pm1)\n",
    "                if(candidate <= D_min):\n",
    "                    D_min = candidate\n",
    "                    self.matchings[row,col,0] = prev[0]\n",
    "                    self.matchings[row,col,1] = prev[1] - pm1\n",
    "                    self.dists[row,col] = D_min\n",
    "        \n",
    "        #now we do the random search step\n",
    "        \n",
    "        i = 0\n",
    "        #maximum random search radius. Set as maximum image dimension\n",
    "        destshape = self.dest.shape\n",
    "        dest_rows = destshape[1]\n",
    "        dest_cols = destshape[2]\n",
    "        w = max(dest_rows,dest_cols)\n",
    "        \n",
    "        #This search step replaces search loop. The loop ends when w*(alpha**i), which is the search radius,\n",
    "        #gets to be less than or equal to 1.\n",
    "        num_iters = int(np.emath.logn(alpha,1/w))\n",
    "        #If it turns out we wouldn't be doing any iterations for some reason, we skip the operations\n",
    "        if(num_iters > 0):\n",
    "            #This part is actually simplified somewhat. Take the current position, get a random 2d offset \n",
    "            #vector where each element is within the range(-w*alpha**i,w*alpha**i). The proper way to do this\n",
    "            #would be to create a search with a radius of #-w*alpha**i.\n",
    "            prods = np.full((num_iters,2),[row,col],dtype = int) + np.rint((np.random.rand(num_iters,2) - 0.5)*2*(w*alpha**np.arange(num_iters).reshape(num_iters,1))).astype(int)\n",
    "            #check if each generated position is a valid one. If not, exclude it\n",
    "            rows_in_bounds = np.logical_and(np.greater_equal(prods[:,0],0),np.less(prods[:,0],dest_rows))\n",
    "            cols_in_bounds = np.logical_and(np.greater_equal(prods[:,1],0),np.less(prods[:,1],dest_cols))\n",
    "            prods = prods[np.logical_and(rows_in_bounds,cols_in_bounds)]\n",
    "            #don't bother getting distances if every option has been excluded\n",
    "            if(len(prods > 0)):\n",
    "                candidate,dist = self.__D_vec([row,col],prods)\n",
    "                #D_vec takes vector of positions and returns the position and distance of the best match out of them\n",
    "                if(dist <= D_min + threshold):\n",
    "                    self.num_switch += 1\n",
    "                    D_min = dist\n",
    "                    self.matchings[row,col] = candidate\n",
    "                    self.dists[row,col] = D_min\n",
    "\n",
    "#         while w*(alpha**i) >= 1:\n",
    "            \n",
    "#             #rounds to nearest integer instead of truncating\n",
    "#             prod = np.array([row,col]).astype(int) + np.rint((np.random.rand(2) - 0.5)*2*(w*alpha**i)).astype(int)\n",
    "#             if(prod[0] >=0 and prod[0] < dest_rows and prod[1] >= 0 and prod[1] < dest_cols):\n",
    "#                 candidate = self.__D(row,col,prod[0],prod[1])\n",
    "#                 if(candidate < D_min):\n",
    "#                     D_min = candidate\n",
    "#                     self.matchings[row,col,0] = prod[0]\n",
    "#                     self.matchings[row,col,1] = prod[1]\n",
    "#             i += 1\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def iterate(self,n = 5,alpha = 0.5):\n",
    "        norm = self.normalised\n",
    "        #Normalised source image, Each location in the source image should be normalised, because according\n",
    "        #to the paper, this makes for better feature matching\n",
    "        pad_dims = ((0,0),(self.patch_size//2,self.patch_size//2),(self.patch_size//2,self.patch_size//2))\n",
    "        if norm:\n",
    "            self.sNorm = torch.Tensor(np.pad(F.normalize(self.source.detach(), dim = 0).numpy(),pad_dims,mode = 'reflect'))\n",
    "            self.dNorm = torch.Tensor(np.pad(F.normalize(self.dest.detach(),dim = 0).numpy(),pad_dims,mode='reflect'))\n",
    "        else:\n",
    "            self.sNorm = torch.Tensor(np.pad(self.source.detach().numpy(),pad_dims,mode = 'reflect'))\n",
    "            self.dNorm = torch.Tensor(np.pad(self.dest.detach().numpy(),pad_dims,mode='reflect'))\n",
    "        #Makes it easier to get patches. Since this is just a view, it does not take much extra space\n",
    "        window_shape = (self.source.shape[0],self.patch_size,self.patch_size)\n",
    "        self.sWin = view_as_windows(self.sNorm.numpy(),window_shape)\n",
    "        self.dWin = view_as_windows(self.dNorm.numpy(),window_shape)\n",
    "        \n",
    "        self.dists = np.full((self.rows,self.cols),np.inf)\n",
    "        \n",
    "        self.num_switch = 0\n",
    "        \n",
    "        \n",
    "        for x in range(n):\n",
    "            print(\"Iteration {}:\".format(x + 1))\n",
    "            for row in range(self.rows):\n",
    "                if(int((row/self.rows)*100)%10 == 0) and (int(((row - 1)/self.rows)*100)%10 != 0):\n",
    "                    print(\"\\t{}%\".format(int(100*row/self.rows)))\n",
    "                for col in range(self.cols):\n",
    "                    #reverse propogation direction on even runs(suggested by original paper)\n",
    "                    self.__step(row,col,alpha,(x + 1)%2 == 0)\n",
    "            if((self.num_switch/(self.rows*self.cols)) < 0.01):\n",
    "                print(\"\\t Mapping Reached.\")\n",
    "                #break\n",
    "            self.num_switch = 0\n",
    "            \n",
    "            \n",
    "        #so that we do not store unnecessary data\n",
    "        del(self.num_switch)\n",
    "        del(self.sNorm)\n",
    "        del(self.dNorm)\n",
    "        del(self.sWin)\n",
    "        del(self.dWin)\n",
    "        del(self.dists)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64eeb69",
   "metadata": {},
   "source": [
    "### A past experiment in trying to restrict the range of pixels certain segments of an image could draw from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f8e7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class restricted_NNF(NNF):\n",
    "    def __init__(self,img_1,img_2,mapping,patch_size = 7, norm = False):\n",
    "        super().__init__(img_1,img_2,patch_size = 7, norm = False)\n",
    "        \n",
    "        scale_factor = self.rows/mapping.shape[0]\n",
    "        if scale_factor != int(scale_factor):\n",
    "            raise(\"Expected integer value scaling. Instead, the image is scaled by {}.\".format(scale_factor))\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mapping = (mapping*scale_factor).repeat(scale_factor,axis = 0).repeat(scale_factor,axis = 1)\n",
    "        self.settle_streak = 0\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def __D(self,row,col,dest_row,dest_col):\n",
    "        dest_rows = self.dest.shape[1]\n",
    "        dest_cols = self.dest.shape[2]\n",
    "        \n",
    "        r = self.patch_size//2\n",
    "        \n",
    "\n",
    "        patch1 = self.sWin[:,row,col].squeeze()\n",
    "        p1s = patch1.shape\n",
    "#         patch2 = self.dNorm[:,r + dest_row - r_top : r + dest_row + r_bott + 1, r + dest_col - r_left : r + dest_col + r_right + 1]\n",
    "        patch2 = self.dWin[:,dest_row,dest_col].squeeze()\n",
    "        p2s = patch2.shape\n",
    "        if(patch1.shape != patch2.shape):\n",
    "            print(p1s,p2s)\n",
    "            print(\"Source dims: ({} rows by {} cols)\\nDest dims: ({} rows by {} cols)\".format(self.rows,self.cols,dest_rows,dest_cols))\n",
    "            print(\"source row: {}\\nsource col: {}\\ndest row: {}\\ndest col:{}\".format(row,col,dest_row,dest_col))\n",
    "            print(\"Source Patch Cols: [{} to {})\".format(col - r_left,col + r_right + 1))\n",
    "            print(\"Source Patch Rows: [{} to {})\".format(row - r_top,row + r_bott + 1))\n",
    "            print(\"Dest Patch Cols: [{} to {})\".format(dest_col - r_left,dest_col + r_right + 1))\n",
    "            print(\"Dest Patch Rows: [{} to {})\".format(dest_row - r_top,dest_row + r_bott + 1))\n",
    "            print(r_left,r_right,r_top,r_bott,self.source[row - r_top:row + r_bott + 1,row - r_left:row + r_right + 1])\n",
    "        \n",
    "\n",
    "        res = float(((patch1 - patch2)**2).sum())**0.5\n",
    "        return res\n",
    "    \n",
    "    def __D_vec(self,source_loc,dest_locs):\n",
    "        #vectorised version of the distance metric, for the random search step\n",
    "        #the patches given by scikit's window view for a given location have the patch's top left corner as the origin.\n",
    "        #However, this is already accounted for since dNorm and sNorm are padded by r\n",
    "        dest_patch_coords = (dest_locs[:,0].reshape(1,dest_locs.shape[0]),dest_locs[:,1].reshape(1,dest_locs.shape[0]))\n",
    "        \n",
    "        dest_patches = self.dWin[:,dest_patch_coords[0],dest_patch_coords[1]][0][0]\n",
    "        #print(dest_patches.shape)\n",
    "        #dest_patches = dest_patches.squeeze()\n",
    "        \n",
    "        source_patch = self.sWin[:,source_loc[0],source_loc[1]].squeeze()\n",
    "        \n",
    "        \n",
    "        dists = ((source_patch - dest_patches)**2).sum(axis = tuple(range(1,len(dest_patches.shape))))**0.5\n",
    "        #print(dists)\n",
    "        closest = np.argmin(dists)\n",
    "        return np.array(dest_locs[closest]).astype(int),float(dists[closest])\n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    def __step(self,row,col,alpha = 0.5,reverse_prop = False,threshold = 0):\n",
    "        \n",
    "        \n",
    "        #setup\n",
    "        dest_row = self.matchings[row,col,0]\n",
    "        dest_col = self.matchings[row,col,1]\n",
    "        #print(dest_row)\n",
    "        D_min = self.dists[row,col]\n",
    "        if(D_min == np.inf):\n",
    "            D_min = self.__D(row,col,dest_row,dest_col)\n",
    "            self.dists[row,col] = D_min\n",
    "        #propogation step: Compare to mappings near previous spot\n",
    "        candidate = None\n",
    "        \n",
    "        #positive one if reverse_prop,negative one if not\n",
    "        pm1 = (reverse_prop * 2) - 1\n",
    "        \n",
    "        #allow propogation across boundaries\n",
    "        \n",
    "        if(not reverse_prop and (row!= 0)) or (reverse_prop and (row!= self.rows - 1)):\n",
    "            prev = self.matchings[row + pm1,col]\n",
    "            if (not reverse_prop and (prev[0] < self.dest.shape[1] - 1)) or (reverse_prop and (prev[0] > 0)):\n",
    "                candidate = self.__D(row,col,prev[0] - pm1,prev[1])\n",
    "                if(candidate <= D_min):\n",
    "                    D_min = candidate\n",
    "                    self.matchings[row,col,0] = prev[0] - pm1\n",
    "                    self.matchings[row,col,1] = prev[1]\n",
    "                    self.dists[row,col] = D_min\n",
    "                \n",
    "        \n",
    "        if(not reverse_prop and (col!= 0)) or (reverse_prop and (col!= self.cols - 1)):\n",
    "            prev = self.matchings[row,col + pm1]\n",
    "            if (not reverse_prop and (prev[1] < self.dest.shape[2] - 1)) or (reverse_prop and (prev[1] > 0)):\n",
    "                candidate = self.__D(row,col,prev[0],prev[1] - pm1)\n",
    "                if(candidate <= D_min):\n",
    "                    D_min = candidate\n",
    "                    self.matchings[row,col,0] = prev[0]\n",
    "                    self.matchings[row,col,1] = prev[1] - pm1\n",
    "                    self.dists[row,col] = D_min\n",
    "        \n",
    "        #now we do the random search step\n",
    "        \n",
    "        i = 0\n",
    "        #maximum random search radius. Set as maximum image dimension\n",
    "        destshape = self.dest.shape\n",
    "        dest_rows = destshape[1]\n",
    "        dest_cols = destshape[2]\n",
    "        w = max(self.scale_factor,2)\n",
    "        \n",
    "        #This search step replaces search loop. The loop ends when w*(alpha**i), which is the search radius,\n",
    "        #gets to be less than or equal to 1.\n",
    "        num_iters = int(np.emath.logn(alpha,1/w))\n",
    "        #If it turns out we wouldn't be doing any iterations for some reason, we skip the operations\n",
    "        if(num_iters > 0):\n",
    "            #This part is actually simplified somewhat. Take the current position, get a random 2d offset \n",
    "            #vector where each element is within the range(-w*alpha**i,w*alpha**i). The proper way to do this\n",
    "            #would be to create a search with a radius of #-w*alpha**i.\n",
    "\n",
    "            \n",
    "            \n",
    "            prods = np.full((num_iters,2),[row,col],dtype = int) + np.rint((np.random.rand(num_iters,2) - 0.5)*2*(w*alpha**np.arange(num_iters).reshape(num_iters,1))).astype(int)\n",
    "            #check if each generated position is a valid one. If not, exclude it\n",
    "            #print(self.rows,self.cols,dest_rows,dest_cols)\n",
    "            \n",
    "            min_rows = np.maximum(self.scale_factor*(prods[:,0]//self.scale_factor),0)\n",
    "            max_rows = np.minimum((prods[:,0]//self.scale_factor + 1)*self.scale_factor,dest_rows)\n",
    "            min_cols = np.maximum(self.scale_factor*(prods[:,1]//self.scale_factor),0)\n",
    "            max_cols = np.minimum((prods[:,1]//self.scale_factor + 1)*self.scale_factor,dest_cols)\n",
    "            \n",
    "            rows_in_bounds = np.logical_and(np.greater_equal(prods[:,0],min_rows),np.less(prods[:,0],max_rows))\n",
    "            cols_in_bounds = np.logical_and(np.greater_equal(prods[:,1],min_cols),np.less(prods[:,1],max_cols))\n",
    "            prods = prods[np.logical_and(rows_in_bounds,cols_in_bounds)]\n",
    "            \n",
    "            #print(prods)\n",
    "            #don't bother getting distances if every option has been excluded\n",
    "            if(len(prods > 0)):\n",
    "                candidate,dist = self.__D_vec([row,col],prods)\n",
    "                #D_vec takes vector of positions and returns the position and distance of the best match out of them\n",
    "                if(dist <= D_min + threshold):\n",
    "                    self.num_switch += 1\n",
    "                    D_min = dist\n",
    "                    self.matchings[row,col] = candidate\n",
    "                    self.dists[row,col] = D_min\n",
    "\n",
    "        return\n",
    "    \n",
    "        \n",
    "    def iterate(self,n = 5,alpha = 0.5):\n",
    "        norm = self.normalised\n",
    "        #Normalised source image, Each location in the source image should be normalised, because according\n",
    "        #to the paper, this makes for better feature matching\n",
    "        pad_dims = ((0,0),(self.patch_size//2,self.patch_size//2),(self.patch_size//2,self.patch_size//2))\n",
    "        if norm:\n",
    "            self.sNorm = torch.Tensor(np.pad(F.normalize(self.source.detach(), dim = 0).numpy(),pad_dims,mode = 'reflect'))\n",
    "            self.dNorm = torch.Tensor(np.pad(F.normalize(self.dest.detach(),dim = 0).numpy(),pad_dims,mode='reflect'))\n",
    "        else:\n",
    "            self.sNorm = torch.Tensor(np.pad(self.source.detach().numpy(),pad_dims,mode = 'reflect'))\n",
    "            self.dNorm = torch.Tensor(np.pad(self.dest.detach().numpy(),pad_dims,mode='reflect'))\n",
    "        #Makes it easier to get patches. Since this is just a view, it does not take much extra space\n",
    "        window_shape = (self.source.shape[0],self.patch_size,self.patch_size)\n",
    "        self.sWin = view_as_windows(self.sNorm.numpy(),window_shape)\n",
    "        self.dWin = view_as_windows(self.dNorm.numpy(),window_shape)\n",
    "        \n",
    "        self.dists = np.full((self.rows,self.cols),np.inf)\n",
    "        \n",
    "        self.num_switch = 0\n",
    "        \n",
    "        \n",
    "        for x in range(n):\n",
    "            print(\"Iteration {}:\".format(x + 1))\n",
    "            for row in range(self.rows):\n",
    "                if(int((row/self.rows)*100)%10 == 0) and (int(((row - 1)/self.rows)*100)%10 != 0):\n",
    "                    print(\"\\t{}%\".format(int(100*row/self.rows)))\n",
    "                for col in range(self.cols):\n",
    "                    #reverse propogation direction on even runs(suggested by original paper)\n",
    "                    self.__step(row,col,alpha,(x + 1)%2 == 0)\n",
    "            if((self.num_switch/(self.rows*self.cols)) < 0.01):\n",
    "                print(\"\\t Mapping Reached.\")\n",
    "                self.settle_streak += 1\n",
    "                if(self.settle_streak >= 3):\n",
    "                    print(\"\\t Reached a streak of 3 settled states. Stopping early.\")\n",
    "                    break\n",
    "            else:\n",
    "                self.settle_streak = 0\n",
    "            self.num_switch = 0\n",
    "            \n",
    "            \n",
    "        #so that we do not store unnecessary data\n",
    "        del(self.num_switch)\n",
    "        del(self.sNorm)\n",
    "        del(self.dNorm)\n",
    "        del(self.sWin)\n",
    "        del(self.dWin)\n",
    "        del(self.dists)\n",
    "        return\n",
    "    \n",
    "            \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14424b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c23ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5087e40a",
   "metadata": {},
   "source": [
    "# Bi-Directional Search(BDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d091a69f",
   "metadata": {},
   "source": [
    "Two versions are offered: The one implemented according to the paper, and one which tries to weight pixels vs the original image based on certainty, with an adjustable threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eb2eb4",
   "metadata": {},
   "source": [
    "### Paper version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a77571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Technically the correct one\n",
    "class BDS:\n",
    "    def __init__(self,source,target,patch_size = 7,search_alpha = 0.5,NNFs = None, norm = False,default_weight = 0.5):\n",
    "        if patch_size%2 == 0:\n",
    "            print(\"Patch size should be odd. Increasing by 1.\")\n",
    "            patch_size = patch_size + 1\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        #source to target map\n",
    "        self.patch_size = patch_size\n",
    "        self.search_alpha = search_alpha\n",
    "        if(NNFs is None):\n",
    "            self.normalised = norm\n",
    "            self.complete = NNF(source,target,patch_size,norm)\n",
    "            self.cohere = NNF(target,source,patch_size,norm)\n",
    "        else:\n",
    "            self.complete,self.cohere = NNFs\n",
    "            self.normalised = self.complete.normalised or self.cohere.normalised\n",
    "        return\n",
    "    \n",
    "    def __rgbToLAB(self,image_arr):\n",
    "        labImage = color.rgb2lab(image_arr,channel_axis = 0)\n",
    "        return labImage\n",
    "    \n",
    "    def __labToRGB(self,lab_arr):\n",
    "        rgbImage = color.lab2rgb(lab_arr, channel_axis = 0)\n",
    "        return rgbImage\n",
    "        \n",
    "    def refine_bidirectional_maps(self, num_iters = 4):\n",
    "        print(\"Refining Completeness map (Source --> Target) for {} iterations...\".format(num_iters))\n",
    "        self.complete.iterate(num_iters,self.search_alpha)\n",
    "        print(\"Getting Coherence map (Target --> Source) for {} iterations...\".format(num_iters))\n",
    "        self.cohere.iterate(num_iters,self.search_alpha)\n",
    "        return\n",
    "    \n",
    "    def get_new_bidirectional_maps(self):\n",
    "        self.complete = NNF(source,target,self.patch_size)\n",
    "        self.cohere = NNF(target,source,self.patch_size)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def __get_matched_patch(self,NNF_mapping,row,col):\n",
    "        #helper function to get the indices for the patch being mapped from and the patch being mapped to.\n",
    "        to_rows = NNF_mapping.dest.shape[1]\n",
    "        to_cols = NNF_mapping.dest.shape[2]\n",
    "        \n",
    "        from_rows = NNF_mapping.source.shape[1]\n",
    "        from_cols = NNF_mapping.source.shape[2]\n",
    "        \n",
    "        to_row = NNF_mapping.matchings[row,col,0]\n",
    "        to_col = NNF_mapping.matchings[row,col,1]\n",
    "        \n",
    "        r = self.patch_size//2\n",
    "        \n",
    "        r_left = min(r,col,to_col)\n",
    "        r_right = min(r,from_cols - (col + 1),to_cols - (to_col + 1))\n",
    "        r_top = min(r,row,to_row)\n",
    "        r_bott = min(r,from_rows - (row + 1),to_rows - (to_row + 1))\n",
    "        \n",
    "        \n",
    "        from_patch = ((row - r_top,row + r_bott + 1),(col - r_left,col + r_right + 1))\n",
    "        to_patch = ((to_row - r_top,to_row + r_bott + 1),(to_col - r_left,to_col + r_right + 1))\n",
    "        \n",
    "        return (from_patch,to_patch)\n",
    "        \n",
    "    def patch_vote(self,complete_cohere_weights = (0.5,0.5),switch_target = True):\n",
    "        \n",
    "        shape = self.target.shape\n",
    "        rows = shape[1]\n",
    "        cols = shape[2]\n",
    "        chans = shape[0]\n",
    "        \n",
    "\n",
    "        \n",
    "        sShape = self.source.shape\n",
    "        sRows = sShape[1]\n",
    "        sCols = sShape[2]\n",
    "        tShape = self.target.shape\n",
    "        tRows = tShape[1]\n",
    "        tCols = tShape[2]\n",
    "        \n",
    "        \n",
    "        Ns = sRows*sCols\n",
    "        Nt = tRows*tCols\n",
    "        \n",
    "        \n",
    "        votes_t_to_s = np.zeros((sShape[0],tRows,tCols))\n",
    "        votes_s_to_t = np.zeros((sShape[0],tRows,tCols))\n",
    "        \n",
    "        num_votes_t_to_s = np.zeros((tRows,tCols))\n",
    "        num_votes_s_to_t = np.zeros((tRows,tCols))\n",
    "\n",
    "        \n",
    "        src = self.source.numpy(force=True)\n",
    "        \n",
    "        for row in range(tRows):\n",
    "            for col in range(tCols):\n",
    "                t_patch,s_patch = self.__get_matched_patch(self.cohere,row,col)\n",
    "                \n",
    "                t_rows,t_cols = t_patch\n",
    "                s_rows,s_cols = s_patch\n",
    "                \n",
    "                t_top,t_bott = t_rows\n",
    "                t_left,t_right = t_cols\n",
    "                s_top,s_bott = s_rows\n",
    "                s_left,s_right = s_cols\n",
    "                \n",
    "                patch = src[:,s_top:s_bott,s_left:s_right]\n",
    "                \n",
    "                votes_t_to_s[:,t_top:t_bott,t_left:t_right] += patch\n",
    "                num_votes_t_to_s[t_top:t_bott,t_left:t_right] += np.ones((patch.shape[1],patch.shape[2]))\n",
    "                \n",
    "        \n",
    "        for row in range(sRows):\n",
    "            for col in range(sCols):\n",
    "                s_patch,t_patch = self.__get_matched_patch(self.complete,row,col)\n",
    "                \n",
    "                s_rows,s_cols = s_patch\n",
    "                t_rows,t_cols = t_patch\n",
    "                \n",
    "                s_top,s_bott = s_rows\n",
    "                s_left,s_right = s_cols\n",
    "                t_top,t_bott = t_rows\n",
    "                t_left,t_right = t_cols\n",
    "                \n",
    "                s_patch = src[:,s_top:s_bott,s_left:s_right]\n",
    "                num_votes_s_to_t[t_top:t_bott,t_left:t_right] += np.ones((s_patch.shape[1],s_patch.shape[2]))\n",
    "                votes_s_to_t[:,t_top:t_bott,t_left:t_right] += s_patch\n",
    "                \n",
    "                \n",
    "        votes_t_to_s*=complete_cohere_weights[0]*2/Nt\n",
    "        votes_s_to_t *=complete_cohere_weights[1]*2/Ns\n",
    "        num = votes_t_to_s + votes_s_to_t\n",
    "        denom = np.maximum(num_votes_s_to_t,1)*complete_cohere_weights[1]*2/Ns + np.maximum(num_votes_t_to_s,1)*complete_cohere_weights[0]*2/Ns\n",
    "        targ_result = torch.Tensor(num/denom)\n",
    "        if(switch_target):\n",
    "            self.target = targ_result\n",
    "        return targ_result\n",
    "                \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f843634d",
   "metadata": {},
   "source": [
    "### Weighted Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb52eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Works best from my tests\n",
    "class BDS:\n",
    "    def __init__(self,source,target,patch_size = 7,search_alpha = 0.5,NNFs = None, norm = False,default_weight = 0.5):\n",
    "        if patch_size%2 == 0:\n",
    "            print(\"Patch size should be odd. Increasing by 1.\")\n",
    "            patch_size = patch_size + 1\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        #source to target map\n",
    "        self.patch_size = patch_size\n",
    "        self.search_alpha = search_alpha\n",
    "        #Plain black image is not a good default. Default to original target image, but still mostly use votes\n",
    "        self.default_weight = default_weight\n",
    "        if(NNFs is None):\n",
    "            self.normalised = norm\n",
    "            self.complete = NNF(source,target,patch_size,norm)\n",
    "            self.cohere = NNF(target,source,patch_size,norm)\n",
    "        else:\n",
    "            self.complete,self.cohere = NNFs\n",
    "            self.normalised = self.complete.normalised or self.cohere.normalised\n",
    "        return\n",
    "    \n",
    "    def __rgbToLAB(self,image_arr):\n",
    "        labImage = color.rgb2lab(image_arr,channel_axis = 0)\n",
    "        return labImage\n",
    "    \n",
    "    def __labToRGB(self,lab_arr):\n",
    "        rgbImage = color.lab2rgb(lab_arr, channel_axis = 0)\n",
    "        return rgbImage\n",
    "        \n",
    "    def refine_bidirectional_maps(self, num_iters = 4):\n",
    "        print(\"Refining Completeness map (Source --> Target) for {} iterations...\".format(num_iters))\n",
    "        self.complete.iterate(num_iters,self.search_alpha)\n",
    "        print(\"Getting Coherence map (Target --> Source) for {} iterations...\".format(num_iters))\n",
    "        self.cohere.iterate(num_iters,self.search_alpha)\n",
    "        return\n",
    "    \n",
    "    def get_new_bidirectional_maps(self):\n",
    "        self.complete = NNF(source,target,self.patch_size)\n",
    "        self.cohere = NNF(target,source,self.patch_size)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def __get_matched_patch(self,NNF_mapping,row,col):\n",
    "        #helper function to get the indices for the patch being mapped from and the patch being mapped to.\n",
    "        to_rows = NNF_mapping.dest.shape[1]\n",
    "        to_cols = NNF_mapping.dest.shape[2]\n",
    "        \n",
    "        from_rows = NNF_mapping.source.shape[1]\n",
    "        from_cols = NNF_mapping.source.shape[2]\n",
    "        \n",
    "        to_row = NNF_mapping.matchings[row,col,0]\n",
    "        to_col = NNF_mapping.matchings[row,col,1]\n",
    "        \n",
    "        r = self.patch_size//2\n",
    "        \n",
    "        r_left = min(r,col,to_col)\n",
    "        r_right = min(r,from_cols - (col + 1),to_cols - (to_col + 1))\n",
    "        r_top = min(r,row,to_row)\n",
    "        r_bott = min(r,from_rows - (row + 1),to_rows - (to_row + 1))\n",
    "        \n",
    "        \n",
    "        from_patch = ((row - r_top,row + r_bott + 1),(col - r_left,col + r_right + 1))\n",
    "        to_patch = ((to_row - r_top,to_row + r_bott + 1),(to_col - r_left,to_col + r_right + 1))\n",
    "        \n",
    "        return (from_patch,to_patch)\n",
    "        \n",
    "    def patch_vote(self,complete_cohere_weights = (0.5,0.5),switch_target = True):\n",
    "        \n",
    "        #Torch tensors store images: [colour,rows,cols]\n",
    "        shape = self.target.shape\n",
    "        rows = shape[1]\n",
    "        cols = shape[2]\n",
    "        chans = shape[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        sShape = self.source.shape\n",
    "        sRows = sShape[1]\n",
    "        sCols = sShape[2]\n",
    "        tShape = self.target.shape\n",
    "        tRows = tShape[1]\n",
    "        tCols = tShape[2]\n",
    "        \n",
    "        \n",
    "        Ns = sRows*sCols\n",
    "        Nt = tRows*tCols\n",
    "        \n",
    "        \n",
    "        votes_t_to_s = np.zeros((sShape[0],tRows,tCols))\n",
    "        votes_s_to_t = np.zeros((sShape[0],tRows,tCols))\n",
    "        \n",
    "        num_votes_t_to_s = np.zeros((tRows,tCols))\n",
    "        num_votes_s_to_t = np.zeros((tRows,tCols))\n",
    "\n",
    "        \n",
    "        src = self.source.numpy(force=True)\n",
    "        \n",
    "        for row in range(tRows):\n",
    "            for col in range(tCols):\n",
    "                t_patch,s_patch = self.__get_matched_patch(self.cohere,row,col)\n",
    "                \n",
    "                t_rows,t_cols = t_patch\n",
    "                s_rows,s_cols = s_patch\n",
    "                \n",
    "                t_top,t_bott = t_rows\n",
    "                t_left,t_right = t_cols\n",
    "                s_top,s_bott = s_rows\n",
    "                s_left,s_right = s_cols\n",
    "                \n",
    "                patch = src[:,s_top:s_bott,s_left:s_right]\n",
    "\n",
    "                votes_t_to_s[:,t_top:t_bott,t_left:t_right] += patch\n",
    "                num_votes_t_to_s[t_top:t_bott,t_left:t_right] += np.ones((patch.shape[1],patch.shape[2]))\n",
    "                \n",
    "                \n",
    "\n",
    "        \n",
    "        for row in range(sRows):\n",
    "            for col in range(sCols):\n",
    "                s_patch,t_patch = self.__get_matched_patch(self.complete,row,col)\n",
    "                \n",
    "                s_rows,s_cols = s_patch\n",
    "                t_rows,t_cols = t_patch\n",
    "                \n",
    "                s_top,s_bott = s_rows\n",
    "                s_left,s_right = s_cols\n",
    "                t_top,t_bott = t_rows\n",
    "                t_left,t_right = t_cols\n",
    "                \n",
    "                s_patch = src[:,s_top:s_bott,s_left:s_right]\n",
    "                num_votes_s_to_t[t_top:t_bott,t_left:t_right] += np.ones((s_patch.shape[1],s_patch.shape[2]))\n",
    "                votes_s_to_t[:,t_top:t_bott,t_left:t_right] += s_patch\n",
    "                \n",
    "        votes_t_to_s/=np.maximum(num_votes_t_to_s,1)#avoid zero division\n",
    "        votes_s_to_t/=np.maximum(num_votes_s_to_t,1)\n",
    "\n",
    "        num_votes_t_to_s *= complete_cohere_weights[1]*(1/Nt) \n",
    "        num_votes_s_to_t *= complete_cohere_weights[0]*(1/Ns) \n",
    "                \n",
    "                \n",
    "        #make the weights add to 1:\n",
    "        #add small epsilon to avoid zero division\n",
    "        vote_sum = num_votes_t_to_s + num_votes_s_to_t + 0.00001\n",
    "                \n",
    "                \n",
    "        num_votes_t_to_s/=vote_sum\n",
    "        num_votes_s_to_t/=vote_sum\n",
    "                \n",
    "        vote_sum = vote_sum/vote_sum.max()#bound from 0 to 1. Basically what is the confidence for a given pixel.\n",
    "        vote_sum = np.power(vote_sum,self.default_weight)\n",
    "                \n",
    "\n",
    "        \n",
    "        #votes = (votes_t_to_s + votes_s_to_t)/2\n",
    "        votes = votes_t_to_s*num_votes_t_to_s + votes_s_to_t*num_votes_s_to_t\n",
    "        votes = votes*vote_sum + self.target.numpy(force = True)*(1 - vote_sum)\n",
    "        targ_result = torch.Tensor(votes)\n",
    "        if(switch_target):\n",
    "            self.target = targ_result\n",
    "        return targ_result\n",
    "                \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f039f9",
   "metadata": {},
   "source": [
    "### Helper functions that aided in testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddbd920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchup(nn):\n",
    "    unloader = transforms.ToPILImage()\n",
    "    dest,src = nn.dest,nn.source\n",
    "    print(dest.shape)\n",
    "    proto = np.zeros(dest.shape)\n",
    "    for row in range(nn.matchings.shape[0]):\n",
    "        #print(row)\n",
    "        for col in range(nn.matchings.shape[1]):\n",
    "            proto[:,nn.matchings[row,col][0],nn.matchings[row,col][1]] = src[:,row,col]\n",
    "    result = torch.Tensor(proto)\n",
    "    plt.imshow(unloader(result))\n",
    "    return result\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872653e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchdown(nn):\n",
    "    unloader = transforms.ToPILImage()\n",
    "    dest,src = nn.dest,nn.source\n",
    "    proto = np.zeros(src.shape)\n",
    "    for row in range(nn.matchings.shape[0]):\n",
    "        #print(row)\n",
    "        for col in range(nn.matchings.shape[1]):\n",
    "            proto[:,row,col] = dest[:,nn.matchings[row,col][0],nn.matchings[row,col][1]]\n",
    "    result = torch.Tensor(proto)\n",
    "    plt.imshow(unloader(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d750e0",
   "metadata": {},
   "source": [
    "# Colour Transfer Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee85a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg19(pretrained=True).features.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3bb105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labToRGB(lab_tensor):\n",
    "        rgbImage = color.lab2rgb(lab_tensor.numpy(force = True), channel_axis = 0)\n",
    "        rgbTensor = torch.Tensor(rgbImage)\n",
    "        return rgbTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee728f6c",
   "metadata": {},
   "source": [
    "### Implements the error for optimising the transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc877520",
   "metadata": {},
   "outputs": [],
   "source": [
    "class error(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(error, self).__init__()\n",
    "\n",
    "    def similarity_loss(self,omega_L,elp,a,b,source,guidance):\n",
    "\n",
    "        T = torch.mul(a,source) + b\n",
    "        #changed the 1 in the loss term to 2 to avoid negative\n",
    "        return torch.sum(omega_L * (2-(elp)) * torch.sum(torch.pow(T - guidance,2),dim = 0))\n",
    "\n",
    "    def smoothness_loss(self, omega_lum, a,b):\n",
    "\n",
    "#         #cut off last row\n",
    "#         omega_lum_above = omega_lum[:-2,1:-1]\n",
    "#         #cut off first row\n",
    "#         omega_lum_below = omega_lum[2:,1:-1]\n",
    "#         #cut off last col\n",
    "#         omega_lum_left = omega_lum[1:-1,:-2]\n",
    "#         #cut off first col\n",
    "#         omega_lum_right = omega_lum[1:-1,2:]\n",
    "        omega_lum_above,omega_lum_below,omega_lum_left,omega_lum_right = omega_lum\n",
    "\n",
    "        left_padding = 1\n",
    "        right_padding = 1\n",
    "        top_padding = 1\n",
    "        bottom_padding = 1\n",
    "        a_padded = F.pad(a,(left_padding,right_padding,top_padding,bottom_padding),mode = 'reflect')\n",
    "        \n",
    "        a_sum_above_below = (a_padded[:,1:,1:-1] - a_padded[:,:-1,1:-1])\n",
    "        a_sum_left_right = (a_padded[:,1:-1,1:] - a_padded[:,1:-1,:-1])\n",
    "        a_sum_above = torch.sum(torch.pow(a_sum_above_below[:,:-1],2),dim = 0)\n",
    "        a_sum_below = torch.sum(torch.pow(a_sum_above_below[:,1:],2),dim = 0)\n",
    "        a_sum_left = torch.sum(torch.pow(a_sum_left_right[:,:,:-1],2),dim = 0)\n",
    "        a_sum_right = torch.sum(torch.pow(a_sum_left_right[:,:,1:],2),dim = 0)\n",
    "\n",
    "        b_padded = F.pad(b,(left_padding,right_padding,top_padding,bottom_padding),mode = 'reflect')\n",
    "        \n",
    "        b_sum_above_below = (b_padded[:,1:,1:-1] - b_padded[:,:-1,1:-1])\n",
    "        b_sum_left_right = (b_padded[:,1:-1,1:] - b_padded[:,1:-1,:-1])\n",
    "        b_sum_above = torch.pow(b_sum_above_below[:,:-1],2)\n",
    "        b_sum_below = torch.pow(-b_sum_above_below[:,1:],2)\n",
    "        b_sum_left = torch.pow(b_sum_left_right[:,:,:-1],2)\n",
    "        b_sum_right = torch.pow(-b_sum_left_right[:,:,1:],2)\n",
    "\n",
    "        res_above = omega_lum_above*(a_sum_above + b_sum_above)\n",
    "        res_below = omega_lum_below*(a_sum_below + b_sum_below)\n",
    "        res_left = omega_lum_left*(a_sum_left + b_sum_left)\n",
    "        res_right = omega_lum_right*(a_sum_right + b_sum_right)\n",
    "\n",
    "        return torch.sum(res_above + res_below + res_left + res_right)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, omega_L,omega_lum,elp,a,b,source,guidance):\n",
    "\n",
    "        return self.similarity_loss(omega_L,elp,a,b,source,guidance) + 0.0000128*self.smoothness_loss(omega_lum,a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d6ff35",
   "metadata": {},
   "source": [
    "### The class which handles the colour transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe2354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class singleImageColourTransfer:\n",
    "    def __init__(self,source_image,style_image,patch_size = 7, normalised = True,replace = None,\n",
    "                 rollingAv = False,sample_feats = False,output_path = \"./outputs\"):\n",
    "        \n",
    "        self.sample_feats = sample_feats\n",
    "        feat_dict = {}\n",
    "        i = 0\n",
    "        max_pools = 0\n",
    "        prev_pool = -1\n",
    "        while(len(feat_dict) < 5 and i < len(vgg)):\n",
    "            current = vgg[i]\n",
    "            if isinstance(current,nn.ReLU):\n",
    "                if(max_pools != prev_pool):\n",
    "                    feat_dict[i] = \"relu-{}_1\".format(max_pools + 1)\n",
    "                    prev_pool += 1\n",
    "            elif isinstance(current,nn.MaxPool2d):\n",
    "                max_pools += 1\n",
    "            i += 1\n",
    "        self.unloader = transforms.ToPILImage()\n",
    "        self.ii = transforms.Compose([transforms.ToTensor()])\n",
    "        self.feature_map = models.feature_extraction.create_feature_extractor(vgg,feat_dict)\n",
    "        self.source = source_image\n",
    "        self.reference = style_image\n",
    "        self.patch_size = patch_size\n",
    "        if(self.patch_size is None):\n",
    "            ps = round(np.sqrt(self.source.shape[1]*self.source.shape[2]/300))\n",
    "            self.patch_size = min(max( 3 , ps+abs(ps%2 - 1) ),7)\n",
    "            print(\"Patch size:\",self.patch_size)\n",
    "        self.normalised = normalised\n",
    "        self.rollingAv = rollingAv\n",
    "        \n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "        #Files that start with \".\" are invisible files like .DS_Store\n",
    "        md = os.listdir(output_path)\n",
    "        \n",
    "        #by default, model number is the earliest not taken model number\n",
    "        mod_set = set()\n",
    "        default_dest = None\n",
    "        for model in md:\n",
    "            if(model[0] == \".\"):\n",
    "                pass\n",
    "            mod_set.add(model)\n",
    "        for i in range(1,len(mod_set) + 2):\n",
    "            if not (\"model_{}\".format(i) in mod_set):\n",
    "                default_dest = \"model_{}\".format(i)\n",
    "                break\n",
    "            \n",
    "        \n",
    "        \n",
    "        self.model_name = None\n",
    "        if replace is None:\n",
    "            self.model_name = default_dest\n",
    "            \n",
    "        else:\n",
    "            self.model_name = replace if isinstance(replace,str) else \"model_{}\".format(replace)\n",
    "            \n",
    "        print(\"Initialising \" + self.model_name + \"...\")\n",
    "        self.output_path = \"{}/{}\".format(output_path,self.model_name)\n",
    "        if not(replace is None):\n",
    "            os.system('rm -rf {}/*'.format(self.output_path))\n",
    "        if not os.path.exists(self.output_path + \"/intermediate_sources\"):\n",
    "            os.makedirs(self.output_path + \"/intermediate_sources\")\n",
    "        if not os.path.exists(self.output_path + \"/guidance\"):\n",
    "            os.makedirs(self.output_path + \"/guidance\")\n",
    "        \n",
    "        self.intermediate_sources = self.output_path + \"/intermediate_sources\"\n",
    "        self.guidance_images = self.output_path + \"/guidance\"\n",
    "        #Intermediate source starts at being the source image\n",
    "        self.S_L = source_image\n",
    "        self.S_Prev = None\n",
    "        print(\"Finished initialisation.\")\n",
    "        return\n",
    "        \n",
    "    \n",
    "    #=========================================================================================================#\n",
    "    \n",
    "    \n",
    "    def __rgbToLAB(self,image_tensor):\n",
    "        labImage = color.rgb2lab(image_tensor.numpy(force = True),channel_axis = 0)\n",
    "        labTensor = torch.Tensor(labImage)\n",
    "        return labTensor\n",
    "    \n",
    "    def __labToRGB(self,lab_tensor):\n",
    "        rgbImage = color.lab2rgb(lab_tensor.numpy(force = True), channel_axis = 0)\n",
    "        rgbTensor = torch.Tensor(rgbImage)\n",
    "        return rgbTensor\n",
    "    \n",
    "    \n",
    "    #=========================================================================================================#\n",
    "    \n",
    "    \n",
    "    def limit_channels(self,image1,image2,num_channels = None):\n",
    "        #Experiment: I would like to see if good results can be achieved even leaving out information\n",
    "        image_channels = image1.shape[0]\n",
    "        \n",
    "        if num_channels is None:\n",
    "            num_channels = round(image_channels/8)\n",
    "        \n",
    "        if(image_channels <= num_channels):\n",
    "            return image1,image2\n",
    "\n",
    "        rng = np.random.default_rng()\n",
    "        indices = rng.choice(image_channels, size=num_channels, replace=False)\n",
    "        \n",
    "        return image1[indices],image2[indices]\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "    #=========================================================================================================#\n",
    "        \n",
    "    def construct_guidance(self,level,iters = 8,com_coh_weights = (8/9,1/9),source_weight = 0.4):\n",
    "        #Intermediate source is S_L\n",
    "        level_name = None\n",
    "        F_LS = None\n",
    "        F_LR = None\n",
    "        if level != 0:\n",
    "            level_name = \"relu-{}_1\".format(level)\n",
    "            bw = lambda x: ii(unloader(x).convert(\"L\").convert(\"RGB\"))\n",
    "            F_LS = self.feature_map(self.S_L)[level_name]\n",
    "            F_LR = self.feature_map(self.reference)[level_name]\n",
    "        else:\n",
    "            F_LS = self.S_L\n",
    "            F_LR = self.reference\n",
    "        \n",
    "        sShape = F_LS.shape\n",
    "        sRows = sShape[1]\n",
    "        sCols = sShape[2]\n",
    "        rShape = F_LR.shape\n",
    "        rRows = rShape[1]\n",
    "        rCols = rShape[2]\n",
    "        \n",
    "        if(self.sample_feats):\n",
    "            F_LS,F_LR = self.limit_channels(F_LS,F_LR)\n",
    "        \n",
    "#         sqLen = int(((sRows*sCols)/2 + (rRows*rCols)/2)**0.5)\n",
    "#         pLen = int(0.08*sqLen)\n",
    "#         pLen = pLen if pLen%2 == 1 else pLen + 1\n",
    "        \n",
    "#         patchsize = int(max(pLen,3))\n",
    "        guidance_map = None\n",
    "        #if(hasattr(self,\"NNF_matchings\")):\n",
    "            #guidance_NNFs = (restricted_NNF(F_LR,F_LS,self.NNF_matchings[0],patch_size = self.patch_size, norm = self.normalised),\n",
    "                            #restricted_NNF(F_LS,F_LR,self.NNF_matchings[1],patch_size = self.patch_size, norm = self.normalised))\n",
    "            #guidance_map = BDS(F_LR,F_LS,patch_size = self.patch_size,NNFs = guidance_NNFs,norm = self.normalised)\n",
    "            \n",
    "        #else:\n",
    "            #guidance_map = BDS(F_LR,F_LS,patch_size = self.patch_size,norm = self.normalised)\n",
    "        guidance_map = BDS(F_LR,F_LS,patch_size = self.patch_size,norm = self.normalised,default_weight = source_weight)\n",
    "        guidance_map.refine_bidirectional_maps(iters)\n",
    "        #if not hasattr(self,\"NNF_matchings\"):\n",
    "         #   self.NNF_matchings = (guidance_map.complete.matchings,guidance_map.cohere.matchings)\n",
    "        \n",
    "\n",
    "        F_G = guidance_map.patch_vote(complete_cohere_weights = com_coh_weights,switch_target = False)\n",
    "        \n",
    "        \n",
    "        print(\"FG shape\", F_G.shape, \"FLS shape\",F_LS.shape)\n",
    "        \n",
    "        \n",
    "        resize_source = transforms.Resize(F_LS.shape[1:])\n",
    "        resize_ref = transforms.Resize(F_LR.shape[1:])\n",
    "        pretrained_nnfs = (guidance_map.complete, guidance_map.cohere)\n",
    "        rr = resize_ref(self.reference)\n",
    "        rs = resize_source(self.S_L)\n",
    "        print(\"fs\", F_LS.shape,\"fr\", F_LR.shape, \"rs\", rs.shape,\"rr\",rr.shape)\n",
    "        guidance_constructor = BDS(rr,rs,patch_size = self.patch_size,\n",
    "                                   NNFs = pretrained_nnfs, norm = self.normalised,default_weight = source_weight)\n",
    "        G = guidance_constructor.patch_vote(complete_cohere_weights = com_coh_weights,switch_target = False)\n",
    "        return (G,rs,F_G,F_LS)\n",
    "    \n",
    "    #def smoothness_loss(self,)\n",
    "    \n",
    "    #TODO: Implement smoothness and nonlocal loss\n",
    "#     def cluster_semantic(self,S):\n",
    "#         sShape = s.shape\n",
    "#         sRows = sShape[1]\n",
    "#         sCols = sShape[2]\n",
    "#         #basically flatten \n",
    "#         cielab = self.__rgbToLAB(self.feature_map(S)[\"relu-5_1\"]).reshape(-1,sRows*sCols)\n",
    "        \n",
    "    #=========================================================================================================#\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    #=========================================================================================================#\n",
    "        \n",
    "    \n",
    "    def initialise_ab(self,G,patch_radius = 1):\n",
    "        window_shape = (G.shape[0],patch_radius*2 + 1,patch_radius*2 + 1)\n",
    "        resize_source = transforms.Resize(G.shape[1:])\n",
    "        \n",
    "        resize_source\n",
    "        \n",
    "        #pad so we can do window view\n",
    "        pad_dims = ((0,0),(patch_radius,patch_radius),(patch_radius,patch_radius))\n",
    "        src_padded = torch.Tensor(np.pad(resize_source(self.__rgbToLAB(self.source)).detach().numpy(),pad_dims,mode = 'reflect'))\n",
    "        g_padded = torch.Tensor(np.pad(G.detach().numpy(),pad_dims,mode = 'reflect'))\n",
    "        \n",
    "        #put window view\n",
    "        sWin = view_as_windows(src_padded.numpy(),window_shape)\n",
    "        gWin = view_as_windows(g_padded.numpy(),window_shape)\n",
    "        \n",
    "        \n",
    "        #Go through and set each section of a as SD of patch in G over SD of patch in S(+epsilon, avoid zero division)\n",
    "        #Go through and set each section of b as mean of patch in G - a at location times mean of S patch\n",
    "        a = np.zeros(G.shape)\n",
    "        b = np.zeros(G.shape)\n",
    "        epsilon = 0.002\n",
    "        axes = (1,2)\n",
    "        for row in range(G.shape[1]):\n",
    "            for col in range(G.shape[2]):\n",
    "                gPatch = gWin[:,row,col].squeeze()\n",
    "                sPatch = sWin[:,row,col].squeeze()\n",
    "                \n",
    "                g_Patch = gPatch.T.reshape((-1,3))\n",
    "                s_Patch = sPatch.T.reshape((-1,3))\n",
    "                \n",
    "                g_std = np.std(g_Patch,axis = 0)\n",
    "                s_std = np.std(s_Patch,axis = 0)\n",
    "                \n",
    "                g_mean = np.mean(g_Patch, axis = 0)\n",
    "                s_mean = np.mean(s_Patch, axis = 0)\n",
    "                \n",
    "#                 g_std = np.array([np.std(gPatch[0]),np.std(gPatch[1]),np.std(gPatch[2])])\n",
    "#                 s_std = np.array([np.std(sPatch[0]),np.std(sPatch[1]),np.std(sPatch[2])])\n",
    "                \n",
    "#                 g_mean = np.array([np.mean(gPatch[0]),np.mean(gPatch[1]),np.mean(gPatch[2])])\n",
    "#                 s_mean = np.array([np.mean(sPatch[0]),np.mean(sPatch[1]),np.mean(sPatch[2])])\n",
    "                \n",
    "                a[:,row,col] = g_std/(s_std + epsilon)\n",
    "                b[:,row,col] = g_mean - a[:,row,col]*s_mean\n",
    "        a = torch.Tensor(a)\n",
    "        a.requires_grad_()\n",
    "        b = torch.Tensor(b)\n",
    "        b.requires_grad_()\n",
    "        return (a,b)\n",
    "    \n",
    "    \n",
    "    #=========================================================================================================#\n",
    "    \n",
    "    \n",
    "    \n",
    "    def transfer_at_level(self,level,optim_iters = 10,nnf_iters = 8,complete_cohere_weights = (4/9,5/9),source_weight = 0.4):\n",
    "        G,_,F_G,F_S = self.construct_guidance(level,iters = nnf_iters,com_coh_weights = complete_cohere_weights,source_weight = source_weight)\n",
    "        guidance_filename = self.guidance_images + \"/guidance_{}.png\".format(level)\n",
    "        intermediate_source_filename = self.intermediate_sources + \"/source_{}.png\".format(level)\n",
    "        \n",
    "        resize_source = transforms.Resize(G.shape[1:])\n",
    "        S = self.__rgbToLAB(resize_source(self.source))\n",
    "        S = S.detach()\n",
    "        G = self.__rgbToLAB(G)\n",
    "        G = G.detach()\n",
    "        F_G\n",
    "        S.requires_grad_(False)\n",
    "        F_G = F.normalize(F_G, dim = 0)\n",
    "        F_G = F_G.detach()\n",
    "        F_S = F.normalize(F_S, dim = 0)\n",
    "        F_S = F_S.detach()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #Create constant input values for similarity term\n",
    "        omega_L = 4**(level - 1)\n",
    "        #British people asking for assistance(it's actually normalised matching error though):\n",
    "        elp = torch.sum(torch.pow((F_S - F_G),2),dim = 0)\n",
    "        \n",
    "        print(\"elp shape\",elp.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #creates constant input value for smoothness term. This value needs to\n",
    "        s_left_padding = 1\n",
    "        s_right_padding = 1\n",
    "        s_top_padding = 1\n",
    "        s_bottom_padding = 1\n",
    "        S_L_channel_padded = F.pad(S[0].reshape(tuple([1] + list(S[0].shape))),\n",
    "                                   (s_left_padding,s_right_padding,s_top_padding,s_bottom_padding),\n",
    "                                   mode = 'reflect').squeeze()\n",
    "\n",
    "        sum_above_below = (S_L_channel_padded[1:,1:-1] - S_L_channel_padded[:-1,1:-1])\n",
    "        sum_left_right = (S_L_channel_padded[1:-1,1:] - S_L_channel_padded[1:-1,:-1])\n",
    "        sum_above = torch.pow(sum_above_below[:-1],2)\n",
    "        sum_below = torch.pow(-sum_above_below[1:],2)\n",
    "        sum_left = torch.pow(sum_left_right[:,:-1],2)\n",
    "        sum_right = torch.pow(-sum_left_right[:,1:],2)\n",
    "        epsilon_lum = 0.00001\n",
    "        alpha = 1.2\n",
    "        #omega_lum = torch.pow(torch.pow(torch.pow(sum_above + sum_below + sum_left + sum_right,0.5),alpha) + epsilon_lum,-1)\n",
    "        omega_lum_above = torch.pow(torch.pow(torch.pow(sum_above,0.5),alpha) + epsilon_lum,-1)\n",
    "        omega_lum_below = torch.pow(torch.pow(torch.pow(sum_below,0.5),alpha) + epsilon_lum,-1)\n",
    "        omega_lum_left = torch.pow(torch.pow(torch.pow(sum_left,0.5),alpha) + epsilon_lum,-1)\n",
    "        omega_lum_right = torch.pow(torch.pow(torch.pow(sum_right,0.5),alpha) + epsilon_lum,-1)\n",
    "        omega_lum = (omega_lum_above,omega_lum_below,omega_lum_left,omega_lum_right)\n",
    "        #omega_lum = F.pad(omega_lum,(1,1,1,1),mode = \"constant\", value = 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        a,b = self.initialise_ab(G)\n",
    "        print(\"A size\", a.shape, \"B size\", b.shape, \"Omega Size\",omega_lum[0].shape, \"S size\", S.shape )\n",
    "        a.requires_grad_(True)\n",
    "        b.requires_grad_(True)\n",
    "        \n",
    "        #T = torch.mul(a,S) + b\n",
    "        #plt.imshow(unloader(labToRGB(T)))\n",
    "        #raise()\n",
    "            \n",
    "        \n",
    "        \n",
    "        optimiser = torch.optim.Adam([a,b],lr = 0.001)#0.001\n",
    "        loss_fn = error()\n",
    "        for i in range(optim_iters):\n",
    "            # Forward pass\n",
    "            loss = loss_fn(omega_L,omega_lum,elp,a,b,S,G)\n",
    "            print(\"\\tLoss: iter {}\\n\\t\\t{}\".format(i,loss))\n",
    "            # Backward and optimize\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        u = nn.Upsample((self.source.shape[1],self.source.shape[2]),mode = \"bilinear\")\n",
    "        upsamp = lambda x: u(x.reshape(tuple([1] + list(x.shape)))).squeeze()\n",
    "        \n",
    "        \n",
    "        a_up = upsamp(a.detach())\n",
    "        b_up = upsamp(b.detach())\n",
    "        self.S_L = self.__labToRGB(torch.mul(a_up,self.__rgbToLAB(self.source)) + b_up)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #self.S_L = torch.div(self.S_L,torch.max(self.S_L))\n",
    "        if (not self.S_Prev is None) and self.rollingAv:\n",
    "            self.S_L = self.S_L*0.4 + self.S_Prev*0.6\n",
    "        if self.S_Prev is None:\n",
    "            self.S_Prev = self.S_L\n",
    "            \n",
    "            \n",
    "        self.unloader(self.__labToRGB(G)).save(guidance_filename)\n",
    "        self.unloader(self.S_L).save(intermediate_source_filename)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    #=========================================================================================================#\n",
    "    \n",
    "    \n",
    "    def transfer(self,optim_iters = 617,nnf_iters = 4,complete_cohere_weights = (2/3,1/3),source_weight = 0.4,\n",
    "                 num_passes = 2,gif = True,gif_length_secs = 5):\n",
    "        im = None\n",
    "        ms = gif_length_secs*1000\n",
    "        gif_name = self.output_path + '/' + self.model_name + '.gif'\n",
    "        gif_folder = gif_name[:-4] + '_temp/'\n",
    "        frame_names = [gif_folder + \"{}.gif\".format(i + 1) for i in range(5*num_passes + 1)]\n",
    "        if(gif):\n",
    "            if not os.path.exists(gif_folder):\n",
    "                os.makedirs(gif_folder)\n",
    "            im = self.unloader(self.source)\n",
    "            im.save(frame_names[0]) \n",
    "        nth = 1\n",
    "        for j in range(num_passes):\n",
    "            for i in range(5,0,-1):\n",
    "                if(i == 5):\n",
    "                    nnf = 42\n",
    "                else:\n",
    "                    nnf = nnf_iters\n",
    "                self.transfer_at_level(i,optim_iters = optim_iters,nnf_iters = nnf,complete_cohere_weights = complete_cohere_weights,\n",
    "                                      source_weight = source_weight)\n",
    "                if gif:\n",
    "                    frame = self.unloader(self.S_L)\n",
    "                    frame.save(frame_names[nth])\n",
    "                    nth += 1\n",
    "            self.source = self.S_L\n",
    "        if gif:\n",
    "            images = []\n",
    "            for n in frame_names:\n",
    "                frame = Image.open(n)\n",
    "                images.append(frame)\n",
    "            for i in range(len(frame_names) - 2, -1, -1):\n",
    "                frame = Image.open(frame_names[i])\n",
    "                images.append(frame)\n",
    "            images.append(self.unloader(self.source))\n",
    "            images[0].save(gif_name,\n",
    "               save_all=True,\n",
    "               append_images=images[1:],\n",
    "               duration=int(ms/(2*len(frame_names) - 1)),\n",
    "               loop=617)\n",
    "            \n",
    "            shutil.rmtree(gif_folder)\n",
    "            \n",
    "            \n",
    "                \n",
    "        return\n",
    "        \n",
    "        \n",
    "        \n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3629d39c",
   "metadata": {},
   "source": [
    "# Testing and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1754d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageSequence\n",
    "\n",
    "def slow_down_gif(path = \"./outputs/model_1/city.gif\",new_length_seconds = 10):\n",
    "    ms = new_length_seconds *1000\n",
    "    im = Image.open(path)\n",
    "    index = 1\n",
    "    images = []\n",
    "    for frame in ImageSequence.Iterator(im):\n",
    "        images.append(frame)\n",
    "        index += 1\n",
    "    os.remove(path)\n",
    "    images[0].save(path,\n",
    "                   save_all=True,\n",
    "                   append_images=images[1:],\n",
    "                   duration=int(ms/len(images)),\n",
    "                   loop=617)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e234c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = \"./source_images/\"\n",
    "ii = transforms.Compose([transforms.ToTensor()])\n",
    "c_night = ii(Image.open(sources + \"city_night.jpg\").convert('RGB'))\n",
    "c_day = ii(Image.open(sources + \"city_day.jpg\").convert('RGB'))\n",
    "resize = transforms.Resize(100)\n",
    "c_night = resize(c_night)\n",
    "c_day = resize(c_day)\n",
    "unloader = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063781a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = unloader(c_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b99bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = \"./source_images/\"\n",
    "car_colours = ii(Image.open(sources + \"有很多顏色的汽車.jpg\").convert('RGB'))\n",
    "yellow_car = ii(Image.open(sources + \"黃色的汽車.jpg\").convert('RGB'))\n",
    "purp_car = ii(Image.open(sources + \"idek.webp\").convert('RGB'))\n",
    "resize = transforms.Resize(100)\n",
    "car_colours = resize(car_colours)\n",
    "yellow_car = resize(yellow_car)\n",
    "purp_car = resize(purp_car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e583cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = \"./source_images/\"\n",
    "cat_geo = ii(Image.open(sources + \"cat_geo.png\").convert('RGB'))\n",
    "orange = ii(Image.open(sources + \"orange.jpg\").convert('RGB'))\n",
    "car_bw = ii(Image.open(sources + \"car_bw.png\").convert('RGB'))\n",
    "\n",
    "resize = transforms.Resize(100)\n",
    "cat_geo = resize(cat_geo)\n",
    "orange = resize(orange)\n",
    "car_bw = resize(car_bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cbbb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = \"./source_images/\"\n",
    "ii = transforms.Compose([transforms.ToTensor()])\n",
    "wren = ii(Image.open(sources + \"house_wren.jpg\").convert('RGB'))\n",
    "cardinal = ii(Image.open(sources + \"northern_cardinal.jpg\").convert('RGB'))\n",
    "resize = transforms.Resize(100)\n",
    "wren = resize(wren)\n",
    "cardinal = resize(cardinal)\n",
    "unloader = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed0b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = \"./source_images/\"\n",
    "ii = transforms.Compose([transforms.ToTensor()])\n",
    "parrot = ii(Image.open(sources + \"parrot.jpg\").convert('RGB'))\n",
    "blackbird = ii(Image.open(sources + \"blackbird.jpg\").convert('RGB'))\n",
    "resize = transforms.Resize(100)\n",
    "parrot = resize(parrot)\n",
    "blackbird = resize(blackbird)\n",
    "unloader = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec7834",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = \"./source_images/\"\n",
    "ii = transforms.Compose([transforms.ToTensor()])\n",
    "sfx_face = ii(Image.open(sources + \"sfx.jpg\").convert('RGB'))\n",
    "no_sfx = ii(Image.open(sources + \"no_sfx.jpg\").convert('RGB'))\n",
    "resize = transforms.Resize(100)\n",
    "sfx_face = resize(sfx_face)\n",
    "no_sfx = resize(no_sfx)\n",
    "unloader = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7c15d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c432df",
   "metadata": {},
   "outputs": [],
   "source": [
    "si = singleImageColourTransfer(blackbird,parrot,patch_size = 7,normalised = True,replace = 1,\n",
    "                               rollingAv = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24f7122",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "si.transfer(optim_iters = int(6170),nnf_iters = 4,\n",
    "            complete_cohere_weights = (0.3,0.7),source_weight = 0.617*0,\n",
    "            num_passes = 1, gif_length_secs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c6f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9b7f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d4076a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ad00e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_to_paper(size = 100,sample = False):\n",
    "    o_path = \"./paper_outputs\"\n",
    "    if(os.path.exists(o_path)):\n",
    "        shutil.rmtree(o_path)\n",
    "    source_path = \"./Paper_Images/\"\n",
    "    num_images = 5\n",
    "    ii = transforms.Compose([transforms.ToTensor()])\n",
    "    resize = transforms.Resize(size)\n",
    "    \n",
    "    get_names = lambda x: (\"in{}.png\".format(x),\"tar{}.png\".format(x))\n",
    "    \n",
    "    transform_image = lambda name: resize(ii(Image.open(source_path + name).convert('RGB')))\n",
    "    \n",
    "    sources = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        src,trg = get_names(i)\n",
    "        sources.append(transform_image(src))\n",
    "        targets.append(transform_image(trg))\n",
    "        \n",
    "    for i in range(num_images):\n",
    "        si = singleImageColourTransfer(sources[i],targets[i],patch_size = 3, normalised = True,replace = None,\n",
    "                                       rollingAv = False,sample_feats = sample,output_path = o_path)\n",
    "        \n",
    "        si.transfer(optim_iters = int(6170),nnf_iters = 4,\n",
    "                    complete_cohere_weights = (0.3,0.7),source_weight = 0.617*0,\n",
    "                    num_passes = 1, gif_length_secs = 10)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e80462",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_paper(256,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae91dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(unloader(ds))\n",
    "plt.title('Source Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(unloader(sr))\n",
    "plt.title('Target Image')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df88ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea264bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this for wls upscale: https://www.cs.huji.ac.il/~danix/epd/epd.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compVis",
   "language": "python",
   "name": "compvis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
